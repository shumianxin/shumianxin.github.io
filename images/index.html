<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shumian Xin</title>

  <meta name="author" content="Shumian Xin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Shumian Xin, an end-to-end product builder shipping model-driven camera and editing experiences at Adobe. Core contributor to Project Indigo (#5 US Photo & Video; 1M downloads in 4 weeks). PhD CMU Robotics; Google Camera internship; CVPR 2019 Best Paper.">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <!-- Header -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Shumian Xin</name>
                </p>

                <p style="text-align:center;margin-top:-10px;">
                  <em>Productizing model-driven software • shipping • quality</em>
                </p>

                <p>
                  I’m a Computer Scientist at Adobe’s Emerging Products (NextCam) team led by
                  <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>.
                  I build and ship consumer camera and editing experiences, and I was a core contributor to
                  <a href="https://apps.apple.com/us/app/project-indigo/id6742591546">Project Indigo</a>, an experimental iOS camera app that reached
                  <strong>#5 in US Photo &amp; Video</strong> and <strong>1M downloads in 4 weeks</strong>.
                </p>

                <p>
                  We operate like a startup inside Adobe, so I own features end-to-end and stay hands-on, spanning problem framing, success criteria,
                  research + engineering implementation, and release quality. The transferable core is productizing performance-constrained, model-driven systems
                  into dependable experiences people trust, and communicating the “why” through clear docs and demos. I’m also exploring agentic, multimodal UX and
                  how to evaluate it for usefulness, reliability, and trust.
                </p>

                <p>
                  <strong>Product training:</strong>
                  <a href="https://digitalcredential.stanford.edu/check/17D5B9EF03EF109ACAC5A7253233B33BE7343E28AE57744866620D4506BCA109WWFEenJEZVQ3bHJrcUNPQ2ZWaEI1eWVZM3RxdXRZZTYxU1lia1hvajVvV2ZNVzFE">
                    Stanford Online: Product Management Certificate (Verified)
                  </a>
                </p>

                <p>
                  Previously, I earned my Ph.D. in Robotics from Carnegie Mellon University, advised by
                  Prof. <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a> and
                  Prof. <a href="https://www.cs.cmu.edu/~srinivas/">Srinivasa Narasimhan</a>.
                  During my PhD, I interned on the Google Camera team, working on dual-pixel computational photography for real-world capture conditions
                  (ICCV 2021 Oral). My non-line-of-sight imaging work received the <strong>CVPR 2019 Best Paper Award</strong>.
                </p>

                <p style="text-align:center">
                  <a href="mailto:xinshumian@gmail.com">Email</a> &nbsp/&nbsp
                  <a href="assets/ShumianXin_CV.pdf">CV</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=XP0f5ogAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/shumianxin/">LinkedIn</a>
                </p>

              </td>

              <td style="padding:2.5%;width:30%;max-width:40%">
                <a href="images/ShumianXin.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShumianXin.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>


        <!-- Products -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Products</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <!-- Project Indigo -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/project_indigo.webp' width="160">
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://apps.apple.com/us/app/project-indigo/id6742591546">
                  <papertitle>Project Indigo: Computational Photography Camera App (Adobe)</papertitle>
                </a>
                <br>

                <p>
                  <strong>#5 in US Photo &amp; Video; 1M downloads in 4 weeks.</strong>
                  Indigo is an experimental iOS camera app for photography enthusiasts, built to deliver a more natural, “SLR-like” look and deeper creative control on mobile.
                </p>

                <p style="margin-top:-6px;">
                  Key experiences include natural HDR/SDR rendering, computational RAW (DNG), pro controls (including computational controls),
                  Lightroom Mobile integration, and a Tech Previews channel (e.g., AI Denoise, Remove Reflections).
                </p>
                <p style="margin-top:-2px;">
                  <strong>My role:</strong> end-to-end feature ownership across capture and editing, from requirements and design tradeoffs to hands-on algorithm
                  development, integration, and tuning, partnering cross-functionally to set the quality/performance/reliability bar and ship reliably at scale.
                </p>

                <a href="https://apps.apple.com/us/app/project-indigo/id6742591546">App Store</a>
                /
                <a href="https://research.adobe.com/articles/indigo/indigo.html">Technical Blog</a>
                /
                <a href="https://www.theverge.com/tech/694014/adobe-project-indigo-camera-app-hands-on-hdr">The Verge</a>
                /
                <a href="https://petapixel.com/2025/06/19/adobes-new-computational-iphone-camera-app-looks-incredible/">PetaPixel</a>
              </td>
            </tr>
          </tbody>
        </table>


        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Publications</heading>
                <p style="margin-top:6px;margin-bottom:-6px;">
                  <em>Work grounded in real-world constraints and user workflows.</em>
                </p>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <!-- Refocus -->
            <tr onmouseout="refocus_stop()" onmouseover="refocus_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refocus_image'>
                    <img src='images/2025_Siggraph_Asia_refocus_output.jpg' width="160"></div>
                    <img src='images/2025_Siggraph_Asia_refocus_input.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function refocus_start() { document.getElementById('refocus_image').style.opacity = "1"; }
                  function refocus_stop() { document.getElementById('refocus_image').style.opacity = "0"; }
                  refocus_stop()
                </script>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://learn2refocus.github.io/">
                  <papertitle>Learning to Refocus with Video Diffusion Models</papertitle>
                </a>
                <br>
                <a href="https://learn2refocus.github.io/">SaiKiran Tedla</a>,
                <a href="https://ztzhang.info/">Zhoutong Zhang</a>,
                <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                <strong>Shumian Xin</strong>
                <br>
                <em>ACM SIGGRAPH Asia</em>, 2025 <br>
                Conference Track
                <br>
                <a href="https://learn2refocus.github.io/">Project webpage</a>
                /
                <a href="https://arxiv.org/abs/2512.19823">Paper</a>
                /
                <a href="https://github.com/tedlasai/learn2refocus">Code</a>
                <p>
                  Diffusion-based refocus for practical post-capture editing: generate a perceptually consistent focal stack from a single defocused photo for interactive refocus,
                  emphasizing controllability, visual consistency, and workflow integration.
                </p>
              </td>
            </tr>

            <!-- Dual Pixel -->
            <tr onmouseout="dualpixel_stop()" onmouseover="dualpixel_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualdefocus_image'>
                    <img src='images/2021_ICCV_dp_defocus_output.png' width="160"></div>
                    <img src='images/2021_ICCV_dp_defocus_input.gif' width="160">
                </div>
                <script type="text/javascript">
                  function dualpixel_start() { document.getElementById('dualdefocus_image').style.opacity = "1"; }
                  function dualpixel_stop() { document.getElementById('dualdefocus_image').style.opacity = "0"; }
                  dualpixel_stop()
                </script>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                  <papertitle>Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</papertitle>
                </a>
                <br>
                <strong>Shumian Xin</strong>,
                <a href="https://nealwadhwa.com/">Neal Wadhwa</a>,
                <a href="http://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                <a href="https://jonbarron.info/">Jonathan Barron</a>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
                <a href="https://rahuldotgarg.appspot.com/">Rahul Garg</a>
                <br>
                <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2021 <br>
                <font color="red"><strong>Oral Presentation</strong></font>
                <br>
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">Project webpage</a>
                /
                <a href="https://imaging.cs.cmu.edu/dual_pixels/assets/dual_pixel_ICCV_2021.pdf">Paper</a>
                /
                <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">Code</a>
                <p>
                  Google Camera (Pixel) internship project: leveraged dual-pixel capture for single-shot defocus estimation and deblurring, improving robustness of post-capture effects under real-world conditions.
                </p>
              </td>
            </tr>

            <!-- NLOS -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/2019_CVPR_fermat_coin_diffuser.png' width="160">
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://imaging.cs.cmu.edu/fermat_paths/">
                  <papertitle>A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction</papertitle>
                </a>
                <br>
                <strong>Shumian Xin</strong>,
                <a href="https://scholar.google.com/citations?user=BIFpV5UAAAAJ&hl=en">Sotiris Nousias</a>,
                <a href="http://www.cs.toronto.edu/~kyros/">Kiriakos N. Kutulakos</a>, <br>
                <a href="https://users.ece.cmu.edu/~saswin/">Aswin C. Sankaranarayanan</a>,
                <a href="https://www.cs.cmu.edu/~srinivas/">Srinivasa G. Narasimhan</a>,
                <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 <br>
                <font color="red"><strong>Oral Presentation, Best Paper Award</strong></font>
                <br>
                <a href="https://imaging.cs.cmu.edu/fermat_paths/">Project webpage</a>
                /
                <a href="https://imaging.cs.cmu.edu/fermat_paths/assets/cvpr2019.pdf">Paper</a>
                /
                <a href="https://github.com/cmu-ci-lab/nlos_fermat_path">Code</a>
                /
                <a href="https://www.youtube.com/watch?v=oxnZNSqCivE&t=18532s&ab_channel=CCD2020">Invited talk</a>
                <p>
                  Developed a theory of Fermat paths for non-line-of-sight (NLOS) shape reconstruction, enabling high-resolution recovery of occluded objects from indirect light transport measurements.
                  Potential applications include around-the-corner perception for robotics and autonomy, search-and-rescue, and inspection in visually obstructed environments.
                </p>
              </td>
            </tr>

          </tbody>
        </table>


        <!-- Footer -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template from <a href="https://jonbarron.info">Jon Barron</a>. Last updated in February 2026.
                </p>
              </td>
            </tr>
          </tbody>
        </table>

      </td>
    </tr>
  </tbody></table>
</body>

</html>