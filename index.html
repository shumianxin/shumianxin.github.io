<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shumian Xin</title>

  <meta name="author" content="Shumian Xin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <!-- Header -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Shumian Xin</name>
                </p>

                <p style="text-align:center;margin-top:-10px;">
                  <em>From prototype to production • evaluation • reliability • adoption</em>
                </p>

                <p>
                  I’m a Computer Scientist at Adobe’s Emerging Products (NextCam) team.
                  I build and ship consumer camera and editing experiences end-to-end, spanning both traditional algorithms and generative AI.
                  I’m a core contributor to
                  <a href="https://apps.apple.com/us/app/project-indigo/id6742591546">Project Indigo</a>, an experimental iOS camera app that reached
                  <strong>#5 in US Photo &amp; Video</strong> and <strong>1M downloads in 4 weeks</strong>.
                </p>

                <p>
                  We operate like a startup inside Adobe, and I own features from problem framing and success criteria to hands-on implementation and release quality,
                  then iterate through instrumentation and user feedback. I care about evaluation, reliability, and adoption, and I translate technical detail into
                  clear docs and demos. I’m currently exploring agentic, multimodal UX and how to evaluate it for usefulness, reliability, and trust.
                </p>

                <p>
                  <strong>Product training:</strong>
                  <a href="https://digitalcredential.stanford.edu/check/17D5B9EF03EF109ACAC5A7253233B33BE7343E28AE57744866620D4506BCA109WWFEenJEZVQ3bHJrcUNPQ2ZWaEI1eWVZM3RxdXRZZTYxU1lia1hvajVvV2ZNVzFE">
                    Stanford Online Product Management Certificate
                  </a>
                </p>

                <p>
                  Previously, I earned my Ph.D. in Robotics from Carnegie Mellon University, advised by
                  Prof. <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a> and
                  Prof. <a href="https://www.cs.cmu.edu/~srinivas/">Srinivasa Narasimhan</a>.
                  During my Ph.D., I interned on the Google Camera team. My non-line-of-sight imaging work received the
                  <strong>CVPR 2019 Best Paper Award</strong>.
                </p>

                <p style="text-align:center">
                  <a href="mailto:xinshumian@gmail.com">Email</a> &nbsp/&nbsp
                  <a href="assets/ShumianXin_CV_202602.pdf">CV</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=XP0f5ogAAAAJ&amp;hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/shumianxin/">LinkedIn</a>
                </p>

              </td>

              <td style="padding:2.5%;width:30%;max-width:40%">
                <a href="images/ShumianXin.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShumianXin.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>


        <!-- Product -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Product</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <!-- Project Indigo -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/project_indigo.webp' width="160">
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://apps.apple.com/us/app/project-indigo/id6742591546">
                  <papertitle>Project Indigo: Computational Photography Camera App</papertitle>
                </a>
                <br>

                <p>
                  Indigo is an experimental iOS camera app for photography enthusiasts, built to deliver a natural SLR-like look and deeper creative control on mobile.
                </p>

                <p style="margin-top:-6px;">
                  Highlights include natural HDR/SDR rendering, computational RAW (DNG), pro controls (including computational controls),
                  Lightroom Mobile integration, and a Tech Previews channel (AI Denoise, Remove Reflections).
                </p>

                <p style="margin-top:-2px;">
                  <strong>My role:</strong> I own features across capture and editing, translating requirements into design tradeoffs and hands-on algorithm development,
                  integration, tuning, and quality validation. I partner cross-functionally to set the quality, performance, and reliability bar and ship reliably at scale.
                </p>

                <a href="https://apps.apple.com/us/app/project-indigo/id6742591546">App Store</a>
                /
                <a href="https://research.adobe.com/articles/indigo/indigo.html">Technical Blog</a>
                /
                <a href="https://www.theverge.com/tech/694014/adobe-project-indigo-camera-app-hands-on-hdr">The Verge</a>
                /
                <a href="https://petapixel.com/2025/06/19/adobes-new-computational-iphone-camera-app-looks-incredible/">PetaPixel</a>
              </td>
            </tr>
          </tbody>
        </table>


        <!-- Selected Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Selected Publications</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <!-- Refocus -->
            <tr onmouseout="refocus_stop()" onmouseover="refocus_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refocus_image'>
                    <img src='images/2025_Siggraph_Asia_refocus_output.jpg' width="160"></div>
                    <img src='images/2025_Siggraph_Asia_refocus_input.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function refocus_start() { document.getElementById('refocus_image').style.opacity = "1"; }
                  function refocus_stop() { document.getElementById('refocus_image').style.opacity = "0"; }
                  refocus_stop()
                </script>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://learn2refocus.github.io/">
                  <papertitle>Learning to Refocus with Video Diffusion Models</papertitle>
                </a>
                <br>
                <a href="https://learn2refocus.github.io/">SaiKiran Tedla</a>,
                <a href="https://ztzhang.info/">Zhoutong Zhang</a>,
                <a href="https://ceciliavision.github.io/">Xuaner Zhang</a>,
                <strong>Shumian Xin</strong>
                <br>
                <em>ACM SIGGRAPH Asia</em>, 2025 <br>
                Conference Track
                <br>
                <a href="https://learn2refocus.github.io/">Project webpage</a>
                /
                <a href="https://arxiv.org/abs/2512.19823">Paper</a>
                /
                <a href="https://github.com/tedlasai/learn2refocus">Code</a>
                <p>
                  Diffusion-based refocus for practical post-capture editing, generating a perceptually consistent focal stack from a single defocused image,
                  with emphasis on controllability, visual consistency, and workflow integration.
                </p>
              </td>
            </tr>

            <!-- Dual Pixel -->
            <tr onmouseout="dualpixel_stop()" onmouseover="dualpixel_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualdefocus_image'>
                    <img src='images/2021_ICCV_dp_defocus_output.png' width="160"></div>
                    <img src='images/2021_ICCV_dp_defocus_input.gif' width="160">
                </div>
                <script type="text/javascript">
                  function dualpixel_start() { document.getElementById('dualdefocus_image').style.opacity = "1"; }
                  function dualpixel_stop() { document.getElementById('dualdefocus_image').style.opacity = "0"; }
                  dualpixel_stop()
                </script>
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                  <papertitle>Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</papertitle>
                </a>
                <br>
                <strong>Shumian Xin</strong>,
                <a href="https://nealwadhwa.com/">Neal Wadhwa</a>,
                <a href="http://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
                <a href="https://jonbarron.info/">Jonathan Barron</a>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
                <a href="https://rahuldotgarg.appspot.com/">Rahul Garg</a>
                <br>
                <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2021 <br>
                <font color="red"><strong>Oral Presentation</strong></font>
                <br>
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">Project webpage</a>
                /
                <a href="https://imaging.cs.cmu.edu/dual_pixels/assets/dual_pixel_ICCV_2021.pdf">Paper</a>
                /
                <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">Code</a>
                <p>
                  Single-shot defocus estimation and deblurring using dual-pixel sensors, jointly estimating a defocus map and reconstructing an all-in-focus image
                  for robust post-capture effects under real-world capture conditions.
                </p>
              </td>
            </tr>

            <!-- NLOS -->
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/2019_CVPR_fermat_coin_diffuser.png' width="160">
              </td>

              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://imaging.cs.cmu.edu/fermat_paths/">
                  <papertitle>A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction</papertitle>
                </a>
                <br>
                <strong>Shumian Xin</strong>,
                <a href="https://scholar.google.com/citations?user=BIFpV5UAAAAJ&amp;hl=en">Sotiris Nousias</a>,
                <a href="http://www.cs.toronto.edu/~kyros/">Kiriakos N. Kutulakos</a>, <br>
                <a href="https://users.ece.cmu.edu/~saswin/">Aswin C. Sankaranarayanan</a>,
                <a href="https://www.cs.cmu.edu/~srinivas/">Srinivasa G. Narasimhan</a>,
                <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019 <br>
                <font color="red"><strong>Oral Presentation, Best Paper Award</strong></font>
                <br>
                <a href="https://imaging.cs.cmu.edu/fermat_paths/">Project webpage</a>
                /
                <a href="https://imaging.cs.cmu.edu/fermat_paths/assets/cvpr2019.pdf">Paper</a>
                /
                <a href="https://github.com/cmu-ci-lab/nlos_fermat_path">Code</a>
                /
                <a href="https://www.youtube.com/watch?v=oxnZNSqCivE&amp;t=18532s&amp;ab_channel=CCD2020">Invited talk</a>
                <p>
                  A theory of Fermat paths for non-line-of-sight shape reconstruction, enabling high-resolution recovery of occluded objects from indirect light transport measurements,
                  with potential applications in robotics and autonomy, search and rescue, and inspection in obstructed environments.
                </p>
              </td>
            </tr>

          </tbody>
        </table>


        <!-- Footer -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template from <a href="https://jonbarron.info">Jon Barron</a>. Last updated in February 2026.
                </p>
              </td>
            </tr>
          </tbody>
        </table>

      </td>
    </tr>
  </tbody></table>
</body>

</html>